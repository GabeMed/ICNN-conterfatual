\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Counterfactual Generation via FICNN Optimization}
\author{}
\date{}

\begin{document}

\maketitle

\section{Problem Formulation}

Given a trained Fully Input Convex Neural Network (FICNN) $f: \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}$ and a factual input $\mathbf{x}_{\text{fact}} \in [0,1]^n$, we seek a counterfactual $\mathbf{x}_{\text{cf}} \in [0,1]^n$ that:
\begin{itemize}
    \item Minimizes the distance to the factual: $\|\mathbf{x}_{\text{cf}} - \mathbf{x}_{\text{fact}}\|_1$
    \item Minimizes the number of changed features (sparsity)
    \item Satisfies the target classification constraint
    \item Respects domain constraints (immutability, one-hot groups, integrality)
\end{itemize}

\section{Decision Variables}

\begin{align}
\mathbf{x} &\in [0,1]^n && \text{Counterfactual features} \\
\mathbf{x}_{\text{fact}} &\in \mathbb{R}^n && \text{Fixed factual features} \\
\boldsymbol{\delta}^+ &\in \mathbb{R}_+^n && \text{Positive deviations} \\
\boldsymbol{\delta}^- &\in \mathbb{R}_+^n && \text{Negative deviations} \\
\mathbf{c} &\in \{0,1\}^n && \text{Binary indicators for changed features} \\
\mathbf{z}^{(l)} &\in \mathbb{R}^{d_l}, \quad l=1,\ldots,L && \text{FICNN hidden layer activations} \\
y_{\text{pred}} &\in [0,1] && \text{FICNN output (prediction)} \\
\boldsymbol{\phi} &\in [0, 0.5]^{n_{\text{cat}}} && \text{Integrality deviation for categoricals}
\end{align}

where:
\begin{itemize}
    \item $n$ = number of features
    \item $n_{\text{num}}$ = number of numeric features (first $n_{\text{num}}$ indices)
    \item $n_{\text{cat}}$ = number of categorical features ($n - n_{\text{num}}$)
    \item $L$ = number of FICNN layers
    \item $d_l$ = dimension of layer $l$
\end{itemize}

\section{FICNN Architecture Constraints}

The FICNN $f(\mathbf{x}, y_{\text{target}})$ is convex in $y$ with skip connections from $\mathbf{x}$ to all layers. We encode it as a feedforward network with ReLU activations using the epigraph formulation.

\subsection{Layer 1 (First Hidden Layer)}

For each neuron $j = 1, \ldots, d_1$:
\begin{align}
a_j^{(1)} &= \sum_{k=1}^{n} W_{jk}^{(x,1)} x_k + \sum_{k=1}^{m} W_{jk}^{(y,1)} y_{\text{target},k} + b_j^{(x,1)} \\
z_j^{(1)} &\geq a_j^{(1)} && \text{(ReLU epigraph)} \\
z_j^{(1)} &\geq 0 && \text{(ReLU non-negativity)}
\end{align}

where:
\begin{itemize}
    \item $W^{(x,1)} \in \mathbb{R}^{d_1 \times n}$ - weights from input $\mathbf{x}$ to layer 1
    \item $W^{(y,1)} \in \mathbb{R}^{d_1 \times m}$ - weights from output $y_{\text{target}}$ to layer 1
    \item $b^{(x,1)} \in \mathbb{R}^{d_1}$ - biases for layer 1
    \item $y_{\text{target}} \in \{0, 1\}^m$ - target output (fixed)
\end{itemize}

\subsection{Hidden Layers $l = 2, \ldots, L-1$}

For each neuron $j = 1, \ldots, d_l$:
\begin{align}
a_j^{(l)} &= \sum_{k=1}^{n} W_{jk}^{(x,l)} x_k + \sum_{k=1}^{d_{l-1}} W_{jk}^{(z,l-1)} z_k^{(l-1)} + \sum_{k=1}^{m} W_{jk}^{(y,l)} y_{\text{target},k} + b_j^{(x,l)} \\
z_j^{(l)} &\geq a_j^{(l)} && \text{(ReLU epigraph)} \\
z_j^{(l)} &\geq 0 && \text{(ReLU non-negativity)}
\end{align}

where $W^{(z,l-1)} \in \mathbb{R}_+^{d_l \times d_{l-1}}$ are **non-negative** weights (enforced during training to maintain convexity).

\subsection{Output Layer $L$}

For the scalar output:
\begin{align}
y_{\text{pred}} &= \sum_{k=1}^{n} W_{k}^{(x,L)} x_k + \sum_{k=1}^{d_{L-1}} W_{k}^{(z,L-1)} z_k^{(L-1)} + \sum_{k=1}^{m} W_{k}^{(y,L)} y_{\text{target},k} + b^{(x,L)} \\
y_{\text{pred}} &\geq 0 \\
y_{\text{pred}} &\leq 1
\end{align}

The output layer is **linear** (no ReLU) and clamped to $[0,1]$ for binary classification.

\section{Distance Decomposition}

The L1 distance is decomposed into positive and negative deviations:
\begin{align}
x_i - x_{\text{fact},i} &= \delta_i^+ - \delta_i^-, \quad \forall i = 1, \ldots, n \\
\delta_i^+, \delta_i^- &\geq 0, \quad \forall i = 1, \ldots, n
\end{align}

Total L1 distance:
\begin{equation}
d(\mathbf{x}, \mathbf{x}_{\text{fact}}) = \sum_{i=1}^{n} (\delta_i^+ + \delta_i^-)
\end{equation}

\section{Sparsity Constraints (Big-M)}

Binary indicators $c_i \in \{0,1\}$ track whether feature $i$ changed:
\begin{align}
\delta_i^+ &\leq M \cdot c_i, \quad \forall i = 1, \ldots, n \\
\delta_i^- &\leq M \cdot c_i, \quad \forall i = 1, \ldots, n
\end{align}

where $M = 1.0$ (since all features are normalized to $[0,1]$).

Number of changed features:
\begin{equation}
\text{sparsity} = \sum_{i=1}^{n} c_i
\end{equation}

\section{Classification Constraint (Margin)}

To ensure the counterfactual is classified as the target class with confidence, we enforce:
\begin{align}
\text{If } y_{\text{target}} = 0: \quad &y_{\text{pred}} \leq 0.5 - \text{margin} \\
\text{If } y_{\text{target}} = 1: \quad &y_{\text{pred}} \geq 0.5 + \text{margin}
\end{align}

where the margin is computed as:
\begin{equation}
\text{margin} = \frac{\alpha - 1}{2\alpha}
\end{equation}

For $\alpha = 2.0$: $\text{margin} = 0.25$ (prediction must be $\leq 0.25$ or $\geq 0.75$).

\section{Domain Constraints}

\subsection{Immutable Features}

For features that cannot be changed (e.g., race, sex, native\_country), indexed by $\mathcal{I} \subseteq \{1, \ldots, n\}$:
\begin{equation}
x_i = x_{\text{fact},i}, \quad \forall i \in \mathcal{I}
\end{equation}

\subsection{One-Hot Group Constraints}

For each categorical feature group $G_k = \{i_1, i_2, \ldots, i_{|G_k|}\}$ (e.g., marital\_status):
\begin{equation}
\sum_{i \in G_k} x_i \leq 1
\end{equation}

This allows at most one category to be active (or none).

\subsection{Integrality Penalty for Categoricals}

To encourage binary values $\{0, 1\}$ for categorical features, we penalize fractional values. For each categorical feature $i \in \mathcal{C}$ (where $\mathcal{C}$ = categorical indices):
\begin{align}
\phi_i &\leq x_i \\
\phi_i &\leq 1 - x_i \\
0 &\leq \phi_i \leq 0.5
\end{align}

This defines $\phi_i = \min(x_i, 1-x_i)$, which measures the distance from the nearest integer. Values close to $0.5$ have high $\phi_i$, while binary values ($0$ or $1$) have $\phi_i = 0$.

\section{Objective Function}

The complete optimization problem minimizes:
\begin{equation}
\boxed{
\min_{\mathbf{x}, \boldsymbol{\delta}^+, \boldsymbol{\delta}^-, \mathbf{c}, \{\mathbf{z}^{(l)}\}, y_{\text{pred}}, \boldsymbol{\phi}} \quad
\underbrace{\sum_{i=1}^{n} (\delta_i^+ + \delta_i^-)}_{\text{L1 distance}} +
\lambda \underbrace{\sum_{i=1}^{n} c_i}_{\text{sparsity}} +
\gamma \underbrace{\sum_{i \in \mathcal{C}} \phi_i}_{\text{integrality}}
}
\end{equation}

where:
\begin{itemize}
    \item $\lambda$ = sparsity weight (e.g., $0.01, 0.1, 1.0$)
    \item $\gamma$ = integrality weight (e.g., $100.0$)
\end{itemize}

\section{Complete MILP Formulation}

\begin{align}
\min \quad & \sum_{i=1}^{n} (\delta_i^+ + \delta_i^-) + \lambda \sum_{i=1}^{n} c_i + \gamma \sum_{i \in \mathcal{C}} \phi_i \\
\text{s.t.} \quad & x_i - x_{\text{fact},i} = \delta_i^+ - \delta_i^-, && \forall i = 1, \ldots, n \\
& \delta_i^+ \leq M \cdot c_i, && \forall i = 1, \ldots, n \\
& \delta_i^- \leq M \cdot c_i, && \forall i = 1, \ldots, n \\
& x_i = x_{\text{fact},i}, && \forall i \in \mathcal{I} \\
& \sum_{i \in G_k} x_i \leq 1, && \forall k \\
& \phi_i \leq x_i, && \forall i \in \mathcal{C} \\
& \phi_i \leq 1 - x_i, && \forall i \in \mathcal{C} \\
& \text{FICNN constraints (Sections 3.1--3.3)} \\
& \text{Classification constraint (Section 5)} \\
& 0 \leq x_i \leq 1, && \forall i = 1, \ldots, n \\
& \delta_i^+, \delta_i^- \geq 0, && \forall i = 1, \ldots, n \\
& c_i \in \{0, 1\}, && \forall i = 1, \ldots, n \\
& 0 \leq \phi_i \leq 0.5, && \forall i \in \mathcal{C} \\
& z_j^{(l)} \geq 0, && \forall j, l \\
& 0 \leq y_{\text{pred}} \leq 1
\end{align}

\section{Model Statistics}

For the Adult Income dataset with 81 features (6 numeric + 75 categorical):
\begin{itemize}
    \item \textbf{Continuous variables}: $81$ (x) + $81$ ($\delta^+$) + $81$ ($\delta^-$) + $75$ ($\phi$) + FICNN hidden ($200 + 200 + 1$) = $719$
    \item \textbf{Binary variables}: $81$ (c) = $81$
    \item \textbf{Constraints}: Distance decomposition ($81$) + Big-M ($162$) + Immutables ($\sim 20$) + One-hot groups ($7$) + Integrality ($150$) + FICNN ReLU ($\sim 800$) + Classification ($1$) $\approx 1221$
\end{itemize}

Solver: Gurobi (MILP), typical solve time: 0.5-2 seconds.

\section{Implementation Notes}

\begin{enumerate}
    \item \textbf{FICNN training}: All features (numeric and categorical) are Float32 in $[0,1]$. Categorical one-hot values are $0.0$ or $1.0$ but stored as continuous.

    \item \textbf{Compatibility}: We keep $\mathbf{x}$ continuous in the optimization to match the trained FICNN. Forcing binary variables causes infeasibility.

    \item \textbf{Integrality penalty}: Soft constraint via penalty term pushes categoricals toward $\{0, 1\}$ without hard enforcement. Weight $\gamma = 100$ provides reasonable trade-off.

    \item \textbf{Normalization}: Features are normalized via Min-Max scaling fitted on training data only (no data leakage). Scaler is saved and applied to test/counterfactual inputs.

    \item \textbf{Sparsity vs Distance}: Larger $\lambda$ reduces number of changed features at the cost of larger L1 distance. Typical values: $0.01$ (few changes), $0.1$ (medium), $1.0$ (sparse).
\end{enumerate}

\end{document}
