"""
Data loader for DC Optimal Power Flow (DCOPF) datasets.

This module loads optimization problems generated by Generate_DCOPF.jl
where:
- X (input) = Demand vectors (concatenated P and Q for each bus)
- Y (output) = Optimal objective values (scalar)

The optimal value is a convex function of the demand, making this suitable
for training Input Convex Neural Networks.
"""

using BSON
using Statistics
using Random

"""
    load_dcopf_data(file_path::String)

Load DCOPF dataset from BSON file generated by Generate_DCOPF.jl.

# Arguments
- `file_path::String`: Path to BSON file containing the dataset

# Returns
A dictionary with keys:
- `"Demand"`: Matrix (n_samples, 2*n_buses) - Input features
- `"ObjDC"`: Vector (n_samples,) - Optimal objective values
- `"DispatchDC"`: Matrix (n_samples, n_generators) - Optimal dispatch
- `"TimeDC"`: Vector (n_samples,) - Solution times
- `"n_valid"`: Number of valid samples

# Example
```julia
data = load_dcopf_data("test_systems/data_pglib_opf_case118_ieee.bson")
X = data["Demand"]
Y = data["ObjDC"]
```
"""
function load_dcopf_data(file_path::String)
    if !isfile(file_path)
        error("File not found: $file_path")
    end

    data = BSON.load(file_path)

    # Validate required keys
    required_keys = ["Demand", "ObjDC"]
    for key in required_keys
        if !haskey(data, key)
            error("Missing required key '$key' in BSON file")
        end
    end

    @info "Loaded DCOPF data from $file_path"
    @info "  Samples: $(size(data["Demand"], 1))"
    @info "  Features: $(size(data["Demand"], 2))"
    @info "  Objective range: [$(minimum(data["ObjDC"])), $(maximum(data["ObjDC"]))]"

    return data
end

"""
    normalize_data(X, Y; method=:standardize)

Normalize input features and output values.

# Arguments
- `X`: Input matrix (n_samples, n_features)
- `Y`: Output vector or matrix (n_samples,) or (n_samples, 1)
- `method`: Normalization method
  - `:standardize` - zero mean, unit variance
  - `:minmax` - scale to [0, 1]
  - `:none` - no normalization

# Returns
- `X_norm`: Normalized input
- `Y_norm`: Normalized output
- `scaler_X`: Dict with normalization parameters for X
- `scaler_Y`: Dict with normalization parameters for Y
"""
function normalize_data(X, Y; method=:standardize)
    X = Float32.(X)
    Y = Float32.(reshape(Y, :, 1))  # Ensure Y is (n_samples, 1)

    scaler_X = Dict{Symbol, Any}()
    scaler_Y = Dict{Symbol, Any}()

    if method == :standardize
        # X normalization
        scaler_X[:mean] = mean(X, dims=1)
        scaler_X[:std] = std(X, dims=1) .+ 1f-8  # Avoid division by zero
        X_norm = (X .- scaler_X[:mean]) ./ scaler_X[:std]

        # Y normalization
        scaler_Y[:mean] = mean(Y)
        scaler_Y[:std] = std(Y) + 1f-8
        Y_norm = (Y .- scaler_Y[:mean]) ./ scaler_Y[:std]

    elseif method == :minmax
        # X normalization
        scaler_X[:min] = minimum(X, dims=1)
        scaler_X[:max] = maximum(X, dims=1)
        X_norm = (X .- scaler_X[:min]) ./ (scaler_X[:max] .- scaler_X[:min] .+ 1f-8)

        # Y normalization
        scaler_Y[:min] = minimum(Y)
        scaler_Y[:max] = maximum(Y)
        Y_norm = (Y .- scaler_Y[:min]) ./ (scaler_Y[:max] .- scaler_Y[:min] + 1f-8)

    elseif method == :none
        X_norm = X
        Y_norm = Y

    else
        error("Unknown normalization method: $method")
    end

    scaler_X[:method] = method
    scaler_Y[:method] = method

    return X_norm, Y_norm, scaler_X, scaler_Y
end

"""
    denormalize_output(Y_norm, scaler_Y)

Denormalize output values back to original scale.

# Arguments
- `Y_norm`: Normalized output
- `scaler_Y`: Scaler parameters from `normalize_data`

# Returns
- `Y`: Denormalized output
"""
function denormalize_output(Y_norm, scaler_Y)
    method = scaler_Y[:method]

    if method == :standardize
        return Y_norm .* scaler_Y[:std] .+ scaler_Y[:mean]
    elseif method == :minmax
        return Y_norm .* (scaler_Y[:max] .- scaler_Y[:min]) .+ scaler_Y[:min]
    else
        return Y_norm
    end
end

"""
    split_data(X, Y; train_ratio=0.8, shuffle=true, seed=42)

Split data into training and testing sets.

# Arguments
- `X`: Input matrix (n_samples, n_features)
- `Y`: Output vector or matrix
- `train_ratio`: Fraction of data for training
- `shuffle`: Whether to shuffle before splitting
- `seed`: Random seed for reproducibility

# Returns
- `X_train, Y_train, X_test, Y_test`
"""
function split_data(X, Y; train_ratio=0.8, shuffle=true, seed=42)
    n_samples = size(X, 1)
    n_train = Int(floor(n_samples * train_ratio))

    if shuffle
        Random.seed!(seed)
        indices = randperm(n_samples)
    else
        indices = 1:n_samples
    end

    train_idx = indices[1:n_train]
    test_idx = indices[n_train+1:end]

    X_train = X[train_idx, :]
    Y_train = Y[train_idx, :]
    X_test = X[test_idx, :]
    Y_test = Y[test_idx, :]

    return X_train, Y_train, X_test, Y_test
end

"""
    prepare_dcopf_dataset(file_path::String;
                         train_ratio=0.8,
                         normalize_method=:standardize,
                         shuffle=true,
                         seed=42)

Complete pipeline to load and prepare DCOPF data for training.

# Arguments
- `file_path`: Path to BSON file
- `train_ratio`: Fraction for training set
- `normalize_method`: :standardize, :minmax, or :none
- `shuffle`: Whether to shuffle data
- `seed`: Random seed

# Returns
Named tuple with:
- `X_train, Y_train`: Training data
- `X_test, Y_test`: Test data
- `scaler_X, scaler_Y`: Normalization parameters
- `n_features`: Number of input features
- `raw_data`: Original data dict

# Example
```julia
dataset = prepare_dcopf_dataset("test_systems/data_case118.bson")
model = FICNN(dataset.n_features, 1; hidden_sizes=[200, 200])
train!(model, dataset.X_train, dataset.Y_train, 100;
       X_test=dataset.X_test, y_test=dataset.Y_test)
```
"""
function prepare_dcopf_dataset(file_path::String;
                              train_ratio=0.8,
                              normalize_method=:standardize,
                              shuffle=true,
                              seed=42)
    # Load raw data
    raw_data = load_dcopf_data(file_path)
    X_raw = raw_data["Demand"]
    Y_raw = raw_data["ObjDC"]

    # Normalize
    X_norm, Y_norm, scaler_X, scaler_Y = normalize_data(X_raw, Y_raw; method=normalize_method)

    # Split
    X_train, Y_train, X_test, Y_test = split_data(X_norm, Y_norm;
                                                   train_ratio=train_ratio,
                                                   shuffle=shuffle,
                                                   seed=seed)

    n_features = size(X_train, 2)

    @info "Dataset prepared:"
    @info "  Training samples: $(size(X_train, 1))"
    @info "  Test samples: $(size(X_test, 1))"
    @info "  Features: $n_features"
    @info "  Normalization: $normalize_method"

    return (
        X_train=X_train,
        Y_train=Y_train,
        X_test=X_test,
        Y_test=Y_test,
        scaler_X=scaler_X,
        scaler_Y=scaler_Y,
        n_features=n_features,
        raw_data=raw_data
    )
end
